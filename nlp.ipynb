{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6f94b641",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4 labels: ['get_exam_hall', 'get_exam_schedule', 'get_hall_info', 'get_seat_location']\n",
      "Train size: 160 | Test size: 40\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader  # <-- FIX IS HERE\n",
    "from torch.optim import AdamW                   # <-- AND HERE\n",
    "from transformers import (\n",
    "    DistilBertTokenizerFast,\n",
    "    DistilBertForSequenceClassification,\n",
    "    get_linear_schedule_with_warmup          # <-- AND HERE\n",
    ")\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import os\n",
    "\n",
    "# --- Load and Process Data ---\n",
    "df = pd.read_csv(\"exam_queries.csv\")\n",
    "\n",
    "# --- Encode Labels ---\n",
    "le = LabelEncoder()\n",
    "df[\"label\"] = le.fit_transform(df[\"intent\"])\n",
    "\n",
    "# **CRITICAL FIX**: Save the correctly ordered label names\n",
    "label_names = list(le.classes_)\n",
    "num_labels = len(label_names)\n",
    "print(f\"Found {num_labels} labels: {label_names}\")\n",
    "\n",
    "# --- Split Data ---\n",
    "train_df, test_df = train_test_split(\n",
    "    df, \n",
    "    test_size=0.2, \n",
    "    random_state=42,\n",
    "    stratify=df[\"label\"]\n",
    ")\n",
    "\n",
    "print(f\"Train size: {len(train_df)} | Test size: {len(test_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0522df82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets created successfully.\n"
     ]
    }
   ],
   "source": [
    "# --- Define Hyperparameters ---\n",
    "MODEL_NAME = \"distilbert-base-uncased\"\n",
    "MAX_LENGTH = 128\n",
    "\n",
    "# --- Load Tokenizer ---\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# --- Tokenize Text ---\n",
    "train_encodings = tokenizer(\n",
    "    list(train_df[\"text\"]),\n",
    "    truncation=True,\n",
    "    padding=True,\n",
    "    max_length=MAX_LENGTH\n",
    ")\n",
    "test_encodings = tokenizer(\n",
    "    list(test_df[\"text\"]),\n",
    "    truncation=True,\n",
    "    padding=True,\n",
    "    max_length=MAX_LENGTH\n",
    ")\n",
    "\n",
    "# --- Create Custom Torch Dataset ---\n",
    "class ExamDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Convert to tensors on-the-fly\n",
    "        item = {key: torch.tensor(val[idx], dtype=torch.long) \n",
    "                for key, val in self.encodings.items()}\n",
    "        item[\"labels\"] = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "train_dataset = ExamDataset(train_encodings, list(train_df[\"label\"]))\n",
    "test_dataset = ExamDataset(test_encodings, list(test_df[\"label\"]))\n",
    "\n",
    "print(\"Datasets created successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "628dcf54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model, Dataloaders, and Optimizer are ready.\n"
     ]
    }
   ],
   "source": [
    "# --- Training Hyperparameters ---\n",
    "EPOCHS = 10\n",
    "LR = 5e-5\n",
    "TRAIN_BATCH_SIZE = 16\n",
    "EVAL_BATCH_SIZE = 32\n",
    "OUTPUT_DIR = \"./exam_intent_model\"\n",
    "\n",
    "# --- Setup Device ---\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# --- Load Model ---\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=num_labels\n",
    ")\n",
    "model.to(device) # Move model to GPU\n",
    "\n",
    "# --- Create Dataloaders ---\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=TRAIN_BATCH_SIZE, \n",
    "    shuffle=True\n",
    ")\n",
    "eval_loader = DataLoader(\n",
    "    test_dataset, \n",
    "    batch_size=EVAL_BATCH_SIZE, \n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# --- Optimizer & Scheduler ---\n",
    "optimizer = AdamW(model.parameters(), lr=LR)\n",
    "total_steps = len(train_loader) * EPOCHS\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=int(0.06 * total_steps), # 6% warmup\n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "print(\"Model, Dataloaders, and Optimizer are ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6949d218",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "Epoch 1/10 â€” train_loss: 1.3652 â€” eval_loss: 1.2344 â€” eval_acc: 0.7750\n",
      "âœ… Saved best model (acc 0.7750) to ./exam_intent_model\n",
      "Epoch 2/10 â€” train_loss: 1.0162 â€” eval_loss: 0.6628 â€” eval_acc: 0.9500\n",
      "âœ… Saved best model (acc 0.9500) to ./exam_intent_model\n",
      "Epoch 3/10 â€” train_loss: 0.4820 â€” eval_loss: 0.3019 â€” eval_acc: 1.0000\n",
      "âœ… Saved best model (acc 1.0000) to ./exam_intent_model\n",
      "Epoch 4/10 â€” train_loss: 0.1973 â€” eval_loss: 0.1505 â€” eval_acc: 1.0000\n",
      "Epoch 5/10 â€” train_loss: 0.0898 â€” eval_loss: 0.0927 â€” eval_acc: 1.0000\n",
      "Epoch 6/10 â€” train_loss: 0.0555 â€” eval_loss: 0.0550 â€” eval_acc: 1.0000\n",
      "Epoch 7/10 â€” train_loss: 0.0405 â€” eval_loss: 0.0457 â€” eval_acc: 1.0000\n",
      "Epoch 8/10 â€” train_loss: 0.0364 â€” eval_loss: 0.0467 â€” eval_acc: 1.0000\n",
      "Epoch 9/10 â€” train_loss: 0.0331 â€” eval_loss: 0.0453 â€” eval_acc: 1.0000\n",
      "Epoch 10/10 â€” train_loss: 0.0315 â€” eval_loss: 0.0438 â€” eval_acc: 1.0000\n",
      "ðŸŽ¯ Training finished. Best eval accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "# --- Training Loop ---\n",
    "best_eval_acc = 0.0\n",
    "print(\"Starting training...\")\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    # ===== Train =====\n",
    "    model.train()\n",
    "    train_losses = []\n",
    "    for batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        train_losses.append(loss.item())\n",
    "    \n",
    "    avg_train_loss = np.mean(train_losses)\n",
    "\n",
    "    # ===== Evaluate =====\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    eval_losses = []\n",
    "    with torch.no_grad():\n",
    "        for batch in eval_loader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss\n",
    "            logits = outputs.logits\n",
    "\n",
    "            preds = torch.argmax(logits, dim=-1).cpu().numpy()\n",
    "            labels = batch[\"labels\"].cpu().numpy()\n",
    "\n",
    "            all_preds.extend(preds.tolist())\n",
    "            all_labels.extend(labels.tolist())\n",
    "            eval_losses.append(loss.item())\n",
    "\n",
    "    avg_eval_loss = np.mean(eval_losses)\n",
    "    eval_acc = accuracy_score(all_labels, all_preds)\n",
    "\n",
    "    print(f\"Epoch {epoch}/{EPOCHS} â€” \"\n",
    "          f\"train_loss: {avg_train_loss:.4f} â€” \"\n",
    "          f\"eval_loss: {avg_eval_loss:.4f} â€” \"\n",
    "          f\"eval_acc: {eval_acc:.4f}\")\n",
    "\n",
    "    # ===== Save Best Model =====\n",
    "    if eval_acc > best_eval_acc:\n",
    "        best_eval_acc = eval_acc\n",
    "        model.save_pretrained(OUTPUT_DIR)\n",
    "        tokenizer.save_pretrained(OUTPUT_DIR)\n",
    "        \n",
    "        # **FIX**: Save the label names in the correct order\n",
    "        with open(os.path.join(OUTPUT_DIR, \"label_names.txt\"), \"w\") as f:\n",
    "            for label in label_names:\n",
    "                f.write(label + \"\\n\")\n",
    "                \n",
    "        print(f\" Saved best model (acc {best_eval_acc:.4f}) to {OUTPUT_DIR}\")\n",
    "\n",
    "print(f\"ðŸŽ¯ Training finished. Best eval accuracy: {best_eval_acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b215c59a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model and labels for inference: ['get_exam_hall', 'get_exam_schedule', 'get_hall_info', 'get_seat_location']\n",
      " Text: Where is my exam hall?\n",
      " Predicted intent: get_exam_hall\n",
      "\n",
      " Text: What is my seat number?\n",
      " Predicted intent: get_seat_location\n",
      "\n",
      " Text: When is my next exam?\n",
      " Predicted intent: get_exam_schedule\n",
      "\n",
      " Text: How many benches are in hall 2?\n",
      " Predicted intent: get_hall_info\n",
      "\n",
      " Text: Tell me my bench for physics\n",
      " Predicted intent: get_seat_location\n",
      "\n",
      " Text: List all available exam halls\n",
      " Predicted intent: get_hall_info\n",
      "\n",
      " Text: Which exam do I have today?\n",
      " Predicted intent: get_exam_schedule\n",
      "\n",
      " Text: Where will I sit for chemistry?\n",
      " Predicted intent: get_seat_location\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- Load Saved Model and Tokenizer ---\n",
    "model_path = \"./exam_intent_model\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "try:\n",
    "    inference_tokenizer = DistilBertTokenizerFast.from_pretrained(model_path)\n",
    "    inference_model = DistilBertForSequenceClassification.from_pretrained(model_path)\n",
    "    inference_model.to(device)\n",
    "    inference_model.eval()\n",
    "\n",
    "    # **THE FIX**: Load the labels from the file we saved during training\n",
    "    # This prevents any mismatch between index and label name\n",
    "    inference_labels = []\n",
    "    with open(os.path.join(model_path, \"label_names.txt\"), \"r\") as f:\n",
    "        inference_labels = [line.strip() for line in f.readlines()]\n",
    "    \n",
    "    print(f\"Loaded model and labels for inference: {inference_labels}\")\n",
    "\n",
    "except OSError:\n",
    "    print(f\"Error: Could not load model from {model_path}. Was training successful?\")\n",
    "    # Stop if the model doesn't exist\n",
    "    raise Exception(\"Model not found\")\n",
    "\n",
    "\n",
    "# --- Prediction Function ---\n",
    "def predict_intent(text):\n",
    "    inputs = inference_tokenizer(\n",
    "        text, \n",
    "        return_tensors=\"pt\", \n",
    "        truncation=True, \n",
    "        padding=True, \n",
    "        max_length=128\n",
    "    )\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = inference_model(**inputs)\n",
    "        probs = F.softmax(outputs.logits, dim=-1)\n",
    "        pred_idx = torch.argmax(probs, dim=-1).item()\n",
    "    \n",
    "    # Map the predicted index back to the *correct* label name\n",
    "    return inference_labels[pred_idx]\n",
    "\n",
    "# --- Test Examples ---\n",
    "test_texts = [\n",
    "    \"Where is my exam hall?\",\n",
    "    \"What is my seat number?\",\n",
    "    \"When is my next exam?\",\n",
    "    \"How many benches are in hall 2?\",\n",
    "    \"Tell me my bench for physics\",\n",
    "    \"List all available exam halls\",\n",
    "    \"Which exam do I have today?\",\n",
    "    \"Where will I sit for chemistry?\"\n",
    "]\n",
    "\n",
    "for txt in test_texts:\n",
    "    pred = predict_intent(txt)\n",
    "    print(f\" Text: {txt}\\n Predicted intent: {pred}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
